{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST with Virtual Workers and Encrypted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "plt.imshow(image[0].view([28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(784, 256)\n",
    "        self.h2 = nn.Linear(256, 128)\n",
    "        self.h3 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.h1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing syft, torch and hooking torch\n",
    "import syft as sy, torch as th\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_dict = {}\n",
    "\n",
    "i = 0\n",
    "for images, labels in trainloader:\n",
    "    worker_name = 'W{}'.format(i)\n",
    "    workers_dict[worker_name] = {}\n",
    "    workers_dict[worker_name]['worker'] = sy.VirtualWorker(hook, id=worker_name)\n",
    "    workers_dict[worker_name]['images'] = images.send(workers_dict[worker_name]['worker'])\n",
    "    workers_dict[worker_name]['labels'] = labels.send(workers_dict[worker_name]['worker'])\n",
    "    i+=1\n",
    "    if i>59: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is not needed because virtual workers already have all other virtual workers added\n",
    "\n",
    "# for worker in workers_dict.values():\n",
    "#     worker['worker'].add_workers([\n",
    "#         w['worker'] for w in workers_dict.values() if w['worker'].id != worker['worker'].id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# secure_worker = sy.VirtualWorker(hook, id='secure_worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying model to each worker\n",
    "for worker in workers_dict.values():\n",
    "    worker['model'] = model.copy().send(worker['worker'])\n",
    "    worker['optimizer'] = optim.SGD(worker['model'].parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "i = 0\n",
    "epoch_loss = 0\n",
    "recent_workers = []\n",
    "\n",
    "for e in range(epoch):\n",
    "    epoch_loss = 0    \n",
    "    \n",
    "    print('Epoch {}'.format(e+1))\n",
    "\n",
    "    # training each workers' models\n",
    "    for worker in workers_dict.values():\n",
    "        \n",
    "        output = worker['model'](worker['images'])\n",
    "        worker['optimizer'].zero_grad()\n",
    "        loss = criterion(output, worker['labels'])\n",
    "        loss.backward()\n",
    "        worker['optimizer'].step()\n",
    "\n",
    "        epoch_loss += loss.get().item()\n",
    "        recent_workers.append(worker['worker'].id)\n",
    "\n",
    "        i+=1        \n",
    "        if i%2 == 0:\n",
    "            \n",
    "            w1 = recent_workers[0]\n",
    "            w2 = recent_workers[1]\n",
    "            \n",
    "            w1h1w = workers_dict[w1]['model'].h1.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1h1b = workers_dict[w1]['model'].h1.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1h2w = workers_dict[w1]['model'].h2.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1h2b = workers_dict[w1]['model'].h2.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1h3w = workers_dict[w1]['model'].h3.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1h3b = workers_dict[w1]['model'].h3.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1outw = workers_dict[w1]['model'].out.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w1outb = workers_dict[w1]['model'].out.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "\n",
    "            w2h1w = workers_dict[w2]['model'].h1.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2h1b = workers_dict[w2]['model'].h1.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2h2w = workers_dict[w2]['model'].h2.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2h2b = workers_dict[w2]['model'].h2.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2h3w = workers_dict[w2]['model'].h3.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2h3b = workers_dict[w2]['model'].h3.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2outw = workers_dict[w2]['model'].out.weight.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "            w2outb = workers_dict[w2]['model'].out.bias.data.fix_prec().get().share(workers_dict[w1]['worker'], workers_dict[w2]['worker'])\n",
    "\n",
    "            with th.no_grad():\n",
    "                \n",
    "                model.h1.weight.set_(((w1h1w + w2h1w)/2).get().float_prec())\n",
    "                model.h1.bias.set_(((w1h1b + w2h1b)/2).get().float_prec())\n",
    "                \n",
    "                model.h2.weight.set_(((w1h2w + w2h2w)/2).get().float_prec())\n",
    "                model.h2.bias.set_(((w1h2b + w2h2b)/2).get().float_prec())\n",
    "                \n",
    "                model.h3.weight.set_(((w1h3w + w2h3w)/2).get().float_prec())\n",
    "                model.h3.bias.set_(((w1h3b + w2h3b)/2).get().float_prec())\n",
    "                \n",
    "                model.out.weight.set_(((w1outw + w2outw)/2).get().float_prec())\n",
    "                model.out.bias.set_(((w1outb + w2outb)/2).get().float_prec())\n",
    "\n",
    "    # copying updated model to virtual workers\n",
    "            for worker in workers_dict.values():\n",
    "                worker['model'] = model.copy().send(worker['worker'])  \n",
    "                worker['optimizer'] = optim.SGD(worker['model'].parameters(), lr=0.01)\n",
    "\n",
    "            recent_workers = []\n",
    "\n",
    "            \n",
    "    print('epoch_loss: {}'.format(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "epoch_loss: 23.041287660598755\n",
      "Epoch 2\n",
      "epoch_loss: 23.034284830093384\n",
      "Epoch 3\n",
      "epoch_loss: 23.027289867401123\n",
      "Epoch 4\n",
      "epoch_loss: 23.020321369171143\n",
      "Epoch 5\n",
      "epoch_loss: 23.013351917266846\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "i = 0\n",
    "n_sharing_workers = 2\n",
    "epoch_loss = 0\n",
    "\n",
    "for e in range(epoch):\n",
    "    epoch_loss = 0    \n",
    "    recent_workers = []\n",
    "    \n",
    "    print('Epoch {}'.format(e+1))\n",
    "\n",
    "    # training each workers' models\n",
    "    for worker in workers_dict.values():\n",
    "        \n",
    "        output = worker['model'](worker['images'])\n",
    "        worker['optimizer'].zero_grad()\n",
    "        loss = criterion(output, worker['labels'])\n",
    "        loss.backward()\n",
    "        worker['optimizer'].step()\n",
    "\n",
    "        epoch_loss += loss.get().item()\n",
    "        recent_workers.append(worker['worker'].id)\n",
    "        i+=1        \n",
    "\n",
    "        if i%n_sharing_workers == 0:        \n",
    "    # fixed precision\n",
    "            for rw in recent_workers:\n",
    "    # i'm bringing it to local to make the averaging work later\n",
    "                workers_dict[rw]['model'] = workers_dict[rw]['model'].get()\n",
    "                \n",
    "                workers_dict[rw]['model'].h1.weight.data = workers_dict[rw]['model'].h1.weight.data.fix_prec()\n",
    "                workers_dict[rw]['model'].h1.bias.data = workers_dict[rw]['model'].h1.bias.data.fix_prec()\n",
    "\n",
    "                workers_dict[rw]['model'].h2.weight.data = workers_dict[rw]['model'].h2.weight.data.fix_prec()\n",
    "                workers_dict[rw]['model'].h2.bias.data = workers_dict[rw]['model'].h2.bias.data.fix_prec()\n",
    "\n",
    "                workers_dict[rw]['model'].h3.weight.data = workers_dict[rw]['model'].h3.weight.data.fix_prec()\n",
    "                workers_dict[rw]['model'].h3.bias.data = workers_dict[rw]['model'].h3.bias.data.fix_prec()\n",
    "\n",
    "                workers_dict[rw]['model'].out.weight.data = workers_dict[rw]['model'].out.weight.data.fix_prec()\n",
    "                workers_dict[rw]['model'].out.bias.data = workers_dict[rw]['model'].out.bias.data.fix_prec()\n",
    "                \n",
    "#                 rw['model'].h1.weight.data = rw['model'].h1.weight.data.fix_prec()\n",
    "#                 rw['model'].h1.bias.data = rw['model'].h1.bias.data.fix_prec()\n",
    "\n",
    "#                 rw['model'].h2.weight.data = rw['model'].h2.weight.data.fix_prec()\n",
    "#                 rw['model'].h2.bias.data = rw['model'].h2.bias.data.fix_prec()\n",
    "\n",
    "#                 rw['model'].h3.weight.data = rw['model'].h3.weight.data.fix_prec()\n",
    "#                 rw['model'].h3.bias.data = rw['model'].h3.bias.data.fix_prec()\n",
    "\n",
    "#                 rw['model'].out.weight.data = rw['model'].out.weight.data.fix_prec()\n",
    "#                 rw['model'].out.bias.data = rw['model'].out.bias.data.fix_prec()\n",
    "                \n",
    "\n",
    "    # sharing recently trained models between workers    \n",
    "            for rw in recent_workers:\n",
    "#                 workers_dict[rw]['model'] = workers_dict[rw]['model'].share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "                \n",
    "                workers_dict[rw]['model'].h1.weight.data = workers_dict[rw]['model'].h1.weight.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "                workers_dict[rw]['model'].h1.bias.data = workers_dict[rw]['model'].h1.bias.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "\n",
    "                workers_dict[rw]['model'].h2.weight.data = workers_dict[rw]['model'].h2.weight.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "                workers_dict[rw]['model'].h2.bias.data = workers_dict[rw]['model'].h2.bias.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "\n",
    "                workers_dict[rw]['model'].h3.weight.data = workers_dict[rw]['model'].h3.weight.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "                workers_dict[rw]['model'].h3.bias.data = workers_dict[rw]['model'].h3.bias.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "\n",
    "                workers_dict[rw]['model'].out.weight.data = workers_dict[rw]['model'].out.weight.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "                workers_dict[rw]['model'].out.bias.data = workers_dict[rw]['model'].out.bias.share(*[workers_dict[w]['worker'] for w in recent_workers])\n",
    "\n",
    "                \n",
    "\n",
    "    # averaging models\n",
    "            \n",
    "#             h1_weights = th.zeros_like(model.h1.weight).fix_prec().child\n",
    "#             for w in recent_workers:\n",
    "#                 h1_weights += 0\n",
    "                \n",
    "#             h1_weights = (recent_workers[0]['model'].h1.weight + recent_workers[1]['model'].h1.weight).float()/2\n",
    "#             h1_weights = h1_weights.get().float_prec()\n",
    "\n",
    "            h1_weights = ((workers_dict[recent_workers[0]]['model'].h1.weight + workers_dict[recent_workers[1]]['model'].h1.weight).float()/2).get().float_prec()\n",
    "            h1_bias = ((workers_dict[recent_workers[0]]['model'].h1.bias + workers_dict[recent_workers[1]]['model'].h1.bias).float()/2).get().float_prec()\n",
    "            h2_weights = ((workers_dict[recent_workers[0]]['model'].h2.weight + workers_dict[recent_workers[1]]['model'].h2.weight).float()/2).get().float_prec()\n",
    "            h2_bias = ((workers_dict[recent_workers[0]]['model'].h2.bias + workers_dict[recent_workers[1]]['model'].h2.bias).float()/2).get().float_prec()\n",
    "            h3_weights = ((workers_dict[recent_workers[0]]['model'].h3.weight + workers_dict[recent_workers[1]]['model'].h3.weight).float()/2).get().float_prec()\n",
    "            h3_bias = ((workers_dict[recent_workers[0]]['model'].h3.bias + workers_dict[recent_workers[1]]['model'].h3.bias).float()/2).get().float_prec()\n",
    "            out_weights = ((workers_dict[recent_workers[0]]['model'].out.weight + workers_dict[recent_workers[1]]['model'].out.weight).float()/2).get().float_prec()\n",
    "            out_bias = ((workers_dict[recent_workers[0]]['model'].out.bias + workers_dict[recent_workers[1]]['model'].out.bias).float()/2).get().float_prec()\n",
    "\n",
    "#             h1_weights = ((recent_workers[0]['model'].h1.weight + recent_workers[1]['model'].h1.weight).float()/2).get().float_prec()\n",
    "#             h1_bias = ((recent_workers[0]['model'].h1.bias + recent_workers[1]['model'].h1.bias).float()/2).get().float_prec()\n",
    "#             h2_weights = ((recent_workers[0]['model'].h2.weight + recent_workers[1]['model'].h2.weight).float()/2).get().float_prec()\n",
    "#             h2_bias = ((recent_workers[0]['model'].h2.bias + recent_workers[1]['model'].h2.bias).float()/2).get().float_prec()\n",
    "#             h3_weights = ((recent_workers[0]['model'].h3.weight + recent_workers[1]['model'].h3.weight).float()/2).get().float_prec()\n",
    "#             h3_bias = ((recent_workers[0]['model'].h3.bias + recent_workers[1]['model'].h3.bias).float()/2).get().float_prec()\n",
    "#             out_weights = ((recent_workers[0]['model'].out.weight + recent_workers[1]['model'].out.weight).float()/2).get().float_prec()\n",
    "#             out_bias = ((recent_workers[0]['model'].out.bias + recent_workers[1]['model'].out.bias).float()/2).get().float_prec()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             h1_weights = th.stack([w['model'].h1.weight for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "#             h1_bias = th.stack([w['model'].h1.bias for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "\n",
    "#             h2_weights = th.stack([w['model'].h2.weight for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "#             h2_bias = th.stack([w['model'].h2.bias for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "\n",
    "#             h3_weights = th.stack([w['model'].h3.weight for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "#             h3_bias = th.stack([w['model'].h3.bias for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "\n",
    "#             out_weights = th.stack([w['model'].out.weight for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "#             out_bias = th.stack([w['model'].out.bias for w in recent_workers]).float().mean(0).get().float_prec()\n",
    "\n",
    "    # updating main model\n",
    "        \n",
    "            with th.no_grad():\n",
    "                \n",
    "                model.h1.weight.set_(h1_weights)\n",
    "                model.h1.bias.set_(h1_bias)\n",
    "                \n",
    "                model.h2.weight.set_(h2_weights)\n",
    "                model.h2.bias.set_(h2_bias)\n",
    "                \n",
    "                model.h3.weight.set_(h3_weights)\n",
    "                model.h3.bias.set_(h3_bias)\n",
    "                \n",
    "                model.out.weight.set_(out_weights)\n",
    "                model.out.bias.set_(out_bias)\n",
    "\n",
    "    # copying updated model to virtual workers\n",
    "            for worker in workers_dict.values():\n",
    "                worker['model'] = model.copy().send(worker['worker'])  \n",
    "                worker['optimizer'] = optim.SGD(worker['model'].parameters(), lr=0.01)\n",
    "\n",
    "            recent_workers = []\n",
    "\n",
    "            \n",
    "    print('epoch_loss: {}'.format(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n",
      "Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "label = labels[0]\n",
    "\n",
    "# plt.imshow(img.view([28,28]))\n",
    "\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "ps = torch.exp(model(img))\n",
    "predicted = ps.argmax().item()\n",
    "\n",
    "print('Label: {}'.format(label))\n",
    "print('Predicted: {}'.format(predicted))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(testloader))\n",
    "# (model(images).argmax(1) == labels).double().mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
