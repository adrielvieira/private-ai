{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST with Virtual Workers and Encrypted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "plt.imshow(image[0].view([28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(784, 256)\n",
    "        self.h2 = nn.Linear(256, 128)\n",
    "        self.h3 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.h1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing syft, torch and hooking torch\n",
    "import syft as sy, torch as th\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_dict = {}\n",
    "\n",
    "i = 0\n",
    "for images, labels in trainloader:\n",
    "    worker_name = 'W{}'.format(i)\n",
    "    workers_dict[worker_name] = {}\n",
    "    workers_dict[worker_name]['worker'] = sy.VirtualWorker(hook, id=worker_name)\n",
    "    workers_dict[worker_name]['images'] = images.send(workers_dict[worker_name]['worker'])\n",
    "    workers_dict[worker_name]['labels'] = labels.send(workers_dict[worker_name]['worker'])\n",
    "    i+=1\n",
    "    if i>40: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is not needed because virtual workers already have all other virtual workers added\n",
    "\n",
    "# for worker in workers_dict.values():\n",
    "#     worker['worker'].add_workers([\n",
    "#         w['worker'] for w in workers_dict.values() if w['worker'].id != worker['worker'].id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "criterion = nn.NLLLoss()\n",
    "optmizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# secure_worker = sy.VirtualWorker(hook, id='secure_worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying model to each worker\n",
    "for worker in workers_dict.values():\n",
    "    worker['model'] = model.copy().send(worker['worker'])\n",
    "    worker['optmizer'] = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "epoch_loss: nan\n",
      "Epoch 2\n",
      "epoch_loss: nan\n",
      "Epoch 3\n",
      "epoch_loss: nan\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "i = 0\n",
    "n_sharing_workers = 10\n",
    "epoch_loss = 0    \n",
    "\n",
    "for e in range(epoch):\n",
    "    epoch_loss = 0    \n",
    "    recent_workers = []\n",
    "    \n",
    "    print('Epoch {}'.format(e+1))\n",
    "\n",
    "    # training each workers' models\n",
    "    for worker in workers_dict.values():\n",
    "        \n",
    "        output = worker['model'](worker['images'])\n",
    "        worker['optmizer'].zero_grad()\n",
    "        loss = criterion(output, worker['labels'])\n",
    "        loss.backward()\n",
    "        worker['optmizer'].step()\n",
    "\n",
    "        epoch_loss += loss.get().item()\n",
    "        recent_workers.append(worker)\n",
    "        i+=1        \n",
    "\n",
    "    # fixed precision\n",
    "    # yet to be implemented\n",
    "\n",
    "    # sharing recently trained models between workers    \n",
    "        if i%n_sharing_workers == 0:        \n",
    "            for rw in recent_workers:\n",
    "                rw['model'] = rw['model'].share(*[w['worker'] for w in recent_workers])\n",
    "\n",
    "    # averaging models\n",
    "\n",
    "            h1_weights = th.stack([w['model'].h1.weight.clone().get() for w in recent_workers]).float().mean(0)\n",
    "            h1_bias = th.stack([w['model'].h1.bias.clone().get() for w in recent_workers]).float().mean(0)\n",
    "\n",
    "            h2_weights = th.stack([w['model'].h2.weight.clone().get() for w in recent_workers]).float().mean(0)\n",
    "            h2_bias = th.stack([w['model'].h2.bias.clone().get() for w in recent_workers]).float().mean(0)\n",
    "\n",
    "            h3_weights = th.stack([w['model'].h3.weight.clone().get() for w in recent_workers]).float().mean(0)\n",
    "            h3_bias = th.stack([w['model'].h3.bias.clone().get() for w in recent_workers]).float().mean(0)\n",
    "\n",
    "            out_weights = th.stack([w['model'].out.weight.clone().get() for w in recent_workers]).float().mean(0)\n",
    "            out_bias = th.stack([w['model'].out.bias.clone().get() for w in recent_workers]).float().mean(0)\n",
    "\n",
    "            recent_workers = []\n",
    "\n",
    "\n",
    "    # updating main model\n",
    "        \n",
    "            with th.no_grad():\n",
    "                model.h1.weight.set_(h1_weights.get())\n",
    "                model.h1.bias.set_(h1_bias.get())\n",
    "                model.h2.weight.set_(h2_weights.get())\n",
    "                model.h2.bias.set_(h2_bias.get())\n",
    "                model.h3.weight.set_(h3_weights.get())\n",
    "                model.h3.bias.set_(h3_bias.get())\n",
    "                model.out.weight.set_(out_weights.get())\n",
    "                model.out.bias.set_(out_bias.get())\n",
    "\n",
    "    # copying updated model to virtual workers\n",
    "            for worker in workers_dict.values():\n",
    "                worker['model'] = model.copy().send(worker['worker'])    \n",
    "            \n",
    "    print('epoch_loss: {}'.format(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n",
      "Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "label = labels[0]\n",
    "\n",
    "# plt.imshow(img.view([28,28]))\n",
    "\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "ps = torch.exp(model(img))\n",
    "predicted = ps.argmax().item()\n",
    "\n",
    "print('Label: {}'.format(label))\n",
    "print('Predicted: {}'.format(predicted))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(testloader))\n",
    "# (model(images).argmax(1) == labels).double().mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
