{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "plt.imshow(image[0].view([28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(784, 256)\n",
    "        self.h2 = nn.Linear(256, 128)\n",
    "        self.h3 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.h1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Data to Virtual Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing syft, torch and hooking torch\n",
    "import syft as sy, torch as th\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_dict = {}\n",
    "i = 0\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    worker_name = 'W{}'.format(i)\n",
    "    workers_dict[worker_name] = {}\n",
    "    workers_dict[worker_name]['worker'] = sy.VirtualWorker(hook, id=worker_name)\n",
    "    workers_dict[worker_name]['images'] = images.send(workers_dict[worker_name]['worker'])\n",
    "    workers_dict[worker_name]['labels'] = labels.send(workers_dict[worker_name]['worker'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not needed because virtual workers already have all other virtual workers added\n",
    "\n",
    "# for worker in workers_dict.values():\n",
    "#     worker['worker'].add_workers([\n",
    "#         w['worker'] for w in workers_dict.values() if w['worker'].id != worker['worker'].id\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "criterion = nn.NLLLoss()\n",
    "optmizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "secure_worker = sy.VirtualWorker(hook, id='secure_worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying model to each worker\n",
    "for worker in workers_dict.values():\n",
    "    worker['model'] = model.copy().send(worker['worker'])\n",
    "    worker['optmizer'] = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # training each workers' models\n",
    "    for worker in workers_dict.values():\n",
    "        \n",
    "        w_model = worker['model']\n",
    "        w_images = worker['images']\n",
    "        w_opt = worker['optmizer']\n",
    "        w_labels = worker['labels']\n",
    "        \n",
    "        output = w_model(w_images)\n",
    "        \n",
    "        w_opt.zero_grad()\n",
    "\n",
    "        loss = criterion(output, w_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        w_opt.step()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # sending models to secure worker\n",
    "        for worker in workers_dict.values():\n",
    "            worker['model'].move(secure_worker)\n",
    "        \n",
    "        # averaging models\n",
    "        h1_weights = th.stack([w['model'].h1.weight.data for w in workers_dict.values()]).mean(0)\n",
    "        h1_bias = th.stack([w['model'].h1.bias.data for w in workers_dict.values()]).mean(0)\n",
    "\n",
    "        h2_weights = th.stack([w['model'].h2.weight.data for w in workers_dict.values()]).mean(0)\n",
    "        h2_bias = th.stack([w['model'].h2.bias.data for w in workers_dict.values()]).mean(0)\n",
    "\n",
    "        h3_weights = th.stack([w['model'].h3.weight.data for w in workers_dict.values()]).mean(0)\n",
    "        h3_bias = th.stack([w['model'].h3.bias.data for w in workers_dict.values()]).mean(0)\n",
    "\n",
    "        out_weights = th.stack([w['model'].out.weight.data for w in workers_dict.values()]).mean(0)\n",
    "        out_bias = th.stack([w['model'].out.bias.data for w in workers_dict.values()]).mean(0)\n",
    "        \n",
    "        # updating local model \n",
    "        with th.no_grad():\n",
    "            model.h1.weight.set_(h1_weights.get())\n",
    "            model.h1.bias.set_(h1_bias.get())\n",
    "            model.h2.weight.set_(h2_weights.get())\n",
    "            model.h2.bias.set_(h2_bias.get())\n",
    "            model.h3.weight.set_(h3_weights.get())\n",
    "            model.h3.bias.set_(h3_bias.get())\n",
    "            model.out.weight.set_(out_weights.get())\n",
    "            model.out.bias.set_(out_bias.get())\n",
    "        \n",
    "        # copying updated model to virtual workers\n",
    "        for worker in workers_dict.values():\n",
    "            worker['model'] = model.copy().send(worker['worker'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 10\n",
    "\n",
    "# for i in range(epoch):\n",
    "#     for worker in workers_dict.values():\n",
    "        \n",
    "#         w_model = worker['model']\n",
    "#         w_images = worker['images']\n",
    "#         w_opt = worker['optmizer']\n",
    "#         w_labels = worker['labels']\n",
    "        \n",
    "#         output = w_model(w_images)\n",
    "        \n",
    "#         w_opt.zero_grad()\n",
    "\n",
    "#         loss = criterion(output, w_labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         w_opt.step()\n",
    "# else:\n",
    "#     for worker in workers_dict.values():\n",
    "#         worker['model'].move(secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1_weights = th.stack([w['model'].h1.weight.data for w in workers_dict.values()]).mean(0)\n",
    "# h1_bias = th.stack([w['model'].h1.bias.data for w in workers_dict.values()]).mean(0)\n",
    "\n",
    "# h2_weights = th.stack([w['model'].h2.weight.data for w in workers_dict.values()]).mean(0)\n",
    "# h2_bias = th.stack([w['model'].h2.bias.data for w in workers_dict.values()]).mean(0)\n",
    "\n",
    "# h3_weights = th.stack([w['model'].h3.weight.data for w in workers_dict.values()]).mean(0)\n",
    "# h3_bias = th.stack([w['model'].h3.bias.data for w in workers_dict.values()]).mean(0)\n",
    "\n",
    "# out_weights = th.stack([w['model'].out.weight.data for w in workers_dict.values()]).mean(0)\n",
    "# out_bias = th.stack([w['model'].out.bias.data for w in workers_dict.values()]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with th.no_grad():\n",
    "#     model.h1.weight.set_(h1_weights.get())\n",
    "#     model.h1.bias.set_(h1_bias.get())\n",
    "#     model.h2.weight.set_(h2_weights.get())\n",
    "#     model.h2.bias.set_(h2_bias.get())\n",
    "#     model.h3.weight.set_(h3_weights.get())\n",
    "#     model.h3.bias.set_(h3_bias.get())\n",
    "#     model.out.weight.set_(out_weights.get())\n",
    "#     model.out.bias.set_(out_bias.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n",
      "Predicted: 3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "label = labels[0]\n",
    "\n",
    "# plt.imshow(img.view([28,28]))\n",
    "\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "ps = torch.exp(model(img))\n",
    "predicted = ps.argmax().item()\n",
    "\n",
    "print('Label: {}'.format(label))\n",
    "print('Predicted: {}'.format(predicted))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.072265625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "(model(images).argmax(1) == labels).double().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
